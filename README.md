<h1>MixtureKit  ğŸš€
<img alt='Leeroo logo' src='https://github.com/MBZUAI-Paris/MixtureKit/blob/main/logo.png' width='220' align='right' />
</h1>

[![made-with-python](https://img.shields.io/badge/Made%20with-Python-green.svg)](#python)

`MixtureKit` package provides high-level helper functions to merge pretrained and finetuned models
into a unified framework, integrating Mixture-of-Experts (MoE) advanced architectures.

## âœ¨ Highlights

* **Oneâ€‘line merge** of multiple HF checkpoints into a single MoE model.
* Supports **Branchâ€‘Trainâ€‘MiX (BTX)**, **Branchâ€‘Trainâ€‘Stitch (BTS)** and *vanilla* MoE.
* Builtâ€‘in **routing visualizer**: inspect which tokens each expert receives â€” overall (coarseâ€‘grained) and per layer (fineâ€‘grained). See [`examples/README_vis.md`](examples/README_vis.md) for details.
---

## Installation

```bash

# Create a fresh conda environment (recommended)
conda create -n mixturekit python=3.12
conda activate mixturekit

# clone & install in editable mode for development
git clone https://github.com/MBZUAI-Paris/MixtureKit
or Download the repo zip file if the git clone does not work
cd MixtureKit
pip install -e .
```

> **Requirements**: PythonÂ â‰¥Â 3.10 Â· PyTorchÂ â‰¥Â 2.5. The correct version of `transformers` is pulled automatically.

---

## Quick start

The script below builds a **BTX** MoE that routes tokens between a Gemmaâ€‘4B base model and two specialized fine-tuned experts (FrenchGemma for *French Language* and MedGemma for *Health Information*). For BTS or vanilla architectures, change the `moe_method` to **BTS** and **traditional** respectively. For other model families, comment the `model_cls`.

```bash
# From the repo root
python examples/example_build_moe.py
```

<details>
<summary>What happens under the hood?</summary>

1. A config dictionary is created that lists the base expert, two additional experts, the routing layers, etc.
2. `MixtureKit.build_moe()` merges the checkpoints and writes the MoE to `models_merge/gemmax/`.
3. The script reloads the model with `AutoModelForCausalLM` and prints a parameterâ€‘breakdown table â€” only router weights stay trainable.

</details>

---
ğŸ”§ Fine-tune / Supervised-Fine-Tuning (SFT)

The **`mixture_training/`** folder contains a ready-to-go scaffold that trains
any merged MoE checkpoint with LoRA-adapters (BTX *or* BTS).

```
mixture_training/
â”œâ”€â”€ config_training.yaml      # all hyper-params in one place
â”œâ”€â”€ deepspeed_config.yaml     # ZeRO-3 config
â”œâ”€â”€ requirements.txt          # extra libs (trl, deepspeed, wandb, etc.)
â””â”€â”€ train_model.py            # launch-script
```

### 1. Prepare your data

* Expected format: ğŸ¤— `datasets` arrow table saved on disk and loaded with
  `load_from_disk()`.
* `config_training.yaml` assumes:
  - a column called **`messages`** (list of chat turns),
  - each turn is a dict `{"role": "...", "content": "..."}`
    (same schema as *ShareGPT*).

### 3. Edit **`config_training.yaml`**

Minimal edits for your own run:


| Key            | What it does                                                       |
| -------------- | ------------------------------------------------------------------ |
| `dataset_path` | Path to the dataset produced in step 2                             |
| `model_id`     | Path or HF-Hub id of the**merged MoE** (e.g. `models_merge/gemmax`) |
| `output_dir`   | Where to write checkpoints / LoRA adapters                         |
| `run_name`     | Friendly name shown in ğŸ¤—`wandb` / logs                            |


### 4. Launch single-node training

```bash
accelerate launch --config_file mixture_training/deepspeed_config.yaml mixture_training/train_model.py
```

The script will:

1. Load the MoE checkpoint in **bf16** with distributed training if multi GPUs are available,
2. Train with ğŸ¤— `trl`â€™s **`SFTTrainer`**,
3. Save incremental checkpoints to the local directory `output_dir`.

> **Tip:** To switch from *BTX*/*Traditional* to *BTS* finetuning, open
> `config_training.yaml` and set `is_bts` to `True`.


## Example gallery

The file [`examples/config_examples.txt`](examples/config_examples.txt) contains more readyâ€‘toâ€‘use configs. Copy one into a small script and call `build_moe()`.

| Key        | Scenario                  | MoE flavour               |
| ---------- | ------------------------- | ------------------------- |
| `llama3x`  | Two Llamaâ€‘3â€‘1B experts    | **BTX**                       |
| `qwen3x`   | Three Qwenâ€‘3â€‘0.6B experts | **Traditional** MoE           |
| `gemmabts` | Gemma + 2 Gemma-experts   | **BTS** (layerâ€‘stitching) |

---

## Documentation

* **API reference** â€” open `docs/index.html` or visit the online version.
---

## Contributing ğŸ¤

Pull requests are welcome! Please open an issue first to discuss your ideas.

---

## License

MixtureKit is released under the **BSD 3-Clause** License â€” see the [LICENSE](./LICENSE) file for details.

---

*Happy mixing!* ğŸ›ï¸
