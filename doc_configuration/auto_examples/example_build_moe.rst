
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/example_build_moe.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_examples_example_build_moe.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_example_build_moe.py:


Building a unified framework with a Mixture-of-Expert (MoE) architecture
========================================================================

This example highlights the substantial steps to merge and integrate the MoE architecture
for futher pretaining and/or finetuning.

We will consider the Llama-family models. However, this example can be applied to any family of models.

.. GENERATED FROM PYTHON SOURCE LINES 12-14

Imports needed for this script
------------------------------

.. GENERATED FROM PYTHON SOURCE LINES 14-28

.. code-block:: Python


    from MixtureKit import build_moe
    import torch
    from transformers import (
        AutoModelForCausalLM,
        AutoModel,
        Gemma3ForCausalLM,
        Gemma3ForConditionalGeneration,
        AutoModel,
        AutoModelForCausalLM,
        Qwen2_5_VLForConditionalGeneration,
        AutoModelForVision2Seq,
    )


.. GENERATED FROM PYTHON SOURCE LINES 29-33

Preparing the configuration dictionary
--------------------------------------
In this part, we prepare the dictionary of the configuration for the unified framework
with all the arguments.

.. GENERATED FROM PYTHON SOURCE LINES 33-67

.. code-block:: Python



    config = {
        "moe_method": "btx",
        "stitch_freq": 5,
        "model_type": "gemmax",
        "num_experts_per_tok": 2,
        "experts": [
            {"expert_name": "base_expert", "model_id": "google/gemma-3-4b-it"},
            {
                "expert_name": "expert_1",
                "model_id": "Sufi2425/FrenchGemma-3-4B-Instruct",
            },
            {
                "expert_name": "expert_2",
                "model_id": "google/medgemma-4b-it",
            },
        ],
        "router_layers": ["mlp.gate_proj", "mlp.up_proj", "mlp.down_proj"],
        "alpha": 0,
        "router_layers_index": [],
    }


    build_moe(
        config=config,
        torch_dtype=torch.bfloat16,
        model_cls=Gemma3ForCausalLM,
    )
    model_btx = AutoModelForCausalLM.from_pretrained(
        "models_merge/gemmax", trust_remote_code=True
    )



.. GENERATED FROM PYTHON SOURCE LINES 68-92

.. code-block:: Python

    total_params = 0
    trainable_params = 0

    print(f"{'Parameter Name':<60} {'Requires Grad':<15} {'Num Params':<15}")
    print("-" * 100)

    for name, param in model_btx.named_parameters():
        num_params = param.numel()
        total_params += num_params

        if "gate." in name:  # keep gate routers trainable
            param.requires_grad_(True)
            trainable_params += num_params
        else:
            param.requires_grad_(False)

        print(
            f"{name:<60} {str(param.requires_grad):<15} {num_params:<15,}"
        )  # formatted with comma

    print("-" * 100)
    print(f"Total parameters: {total_params:,}")
    print(f"Trainable parameters: {trainable_params:,}")
    print(f"Frozen parameters: {total_params - trainable_params:,}")


.. _sphx_glr_download_auto_examples_example_build_moe.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: example_build_moe.py <example_build_moe.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: example_build_moe.zip <example_build_moe.zip>`
