# config = {
#     # The model_type for the new build under the unified framework
#     "moe_method": "btx",
#     "model_type": "llama3x",
#     # The number of active experts per token
#     "num_experts_per_tok": 2,
#     # A list for each expert with its corresponding name and model_id
#     # One is the base_expert and the remainings are the enumerated experts
#     "experts": [
#         {"expert_name": "base_expert", "model_id": "meta-llama/Llama-3.2-1B-Instruct"},
#         {
#             "expert_name": "expert_1",
#             "model_id": "cognitivecomputations/Dolphin3.0-Llama3.2-1B",
#         },
#     ],
#     # The layers to be converted to the MoE architecture with a prefix either mlp (for FFN part)
#     # or attn (for attention part)
#     "router_layers": ["mlp.gate_proj", "mlp.up_proj", "mlp.down_proj"],
#     # "router_layers": ["mlp.up_proj"],
#     # "router_layers": ["self_attn.q_proj", "self_attn.k_proj", "self_attn.v_proj"],
#     # Optional argument that specifies, if needed, which transformer blocks are to apply the moe
#     # conversion to
#     "alpha": 0,
#     "router_layers_index": []
# }

# build_moe(config=config, torch_dtype=torch.bfloat16, moe_architecture="btx", model_cls=AutoModelForCausalLM)
# model_btx = AutoModelForCausalLM.from_pretrained("models_merge/llama3x", trust_remote_code=True)


# config = {
#     # The model_type for the new build under the unified framework
#     "moe_method": "traditional",
#     "model_type": "qwen3x",
#     # The number of active experts per token
#     "num_experts_per_tok": 2,
#     # A list for each expert with its corresponding name and model_id
#     # One is the base_expert and the remainings are the enumerated experts
#     "experts": [
#         {"expert_name": "base_expert", "model_id": "Qwen/Qwen3-0.6B"},
#         {
#             "expert_name": "expert_1",
#             "model_id": "Qwen/Qwen3-0.6B",
#         },
#         {
#             "expert_name": "expert_2",
#             "model_id": "Qwen/Qwen3-0.6B",
#         },
#     ],
#     # The layers to be converted to the MoE architecture with a prefix either mlp (for FFN part)
#     # or attn (for attention part)
#     "router_layers": ["mlp.gate_proj", "mlp.up_proj", "mlp.down_proj"],
#     "alpha": 0.01,
#     "router_layers_index": []
# }

# build_moe(config=config, torch_dtype=torch.bfloat16, moe_architecture="btx", model_cls=AutoModelForCausalLM)
# model_btx = AutoModelForCausalLM.from_pretrained("models_merge/qwen3x", trust_remote_code=True)


# config = {
#     "moe_method": "bts",
#     "stitch_freq": 5,
#     "model_type": "gemmabts",
#     "num_experts_per_tok": 2,
#     "experts": [
#         {"expert_name": "base_expert", "model_id": "google/gemma-3-4b-it"},
#         {"expert_name": "expert_1", "model_id": "Sufi2425/FrenchGemma-3-4B-Instruct"},
#         {"expert_name": "expert_2", "model_id": "google/medgemma-4b-it"},
#     ],
#     "router_layers": [],
#     "alpha": 0,
#     "router_layers_index": []
# }

# build_moe(config=config, torch_dtype=torch.bfloat16, moe_architecture="bts", model_cls=Gemma3ForCausalLM)
# model_btx = AutoModelForCausalLM.from_pretrained("models_merge/nilexbts", trust_remote_code=True)